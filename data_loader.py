import pandas as pd
import glob
import os
import numpy as np

# --- Constants ---
FRENCH_DAYS = {
    'Monday': 'Lundi', 
    'Tuesday': 'Mardi', 
    'Wednesday': 'Mercredi',
    'Thursday': 'Jeudi', 
    'Friday': 'Vendredi', 
    'Saturday': 'Samedi', 
    'Sunday': 'Dimanche'
}

import json

def load_data(base_path="."):
    """
    Loads traffic data.
    Priority 1: Optimized Parquet store (generated by build_dataset.py)
    Priority 2: Raw CSV files in base_path
    """
    metadata = {}
    parquet_path = os.path.join(base_path, "data", "parquet_store")
    
    # Try loading metadata
    # Metadata is stored in the parent 'data' folder
    meta_path = os.path.join(base_path, "data", "metadata.json")
    if os.path.exists(meta_path):
        try:
            with open(meta_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        except Exception as e:
            print(f"Erreur lecture metadata: {e}")

    # Load sites coords
    sites_path = os.path.join(base_path, "data", "sites.json")
    if os.path.exists(sites_path):
        try:
            with open(sites_path, 'r', encoding='utf-8') as f:
                sites_data = json.load(f)
                site_name = metadata.get('site_name')
                if site_name and site_name in sites_data:
                    coords = sites_data[site_name]
                    metadata.update(coords)
                    print(f"Coordonnées chargées pour {site_name}: {coords}")
        except Exception as e:
            print(f"Erreur lecture sites.json: {e}")

    # 1. Try Loading Optimized Parquet Store
    if os.path.exists(parquet_path):
        try:
            print(f"Chargement des données optimisées depuis {parquet_path}...")
            df = pd.read_parquet(parquet_path)
            if not df.empty:
                df.attrs['metadata'] = metadata
                return df
        except Exception as e:
            print(f"Échec lecture Parquet ({e}). Passage aux fichiers CSV...")

    # 2. Raw CSV Loading (Fallback)
    # Generic pattern to find any traffic CSVs, not just Restefond
    pattern = os.path.join(base_path, "*.csv")
    candidate_files = glob.glob(pattern)
    
    # Filter for files that look like traffic data (contain 'WebTraffic' or specific structure)
    files = [f for f in candidate_files if "WebTraffic" in os.path.basename(f)]
    
    if not files:
        print(f"No traffic files found in {base_path}")
        return pd.DataFrame()

    dfs = []
    
    for file in files:
        print(f"Loading {file}...")
        try:
            # Read all columns; avoid low_memory issues
            df = pd.read_csv(file, sep=';', encoding='latin1', on_bad_lines='skip', low_memory=False)
            
            # Identify relevant columns dynamically
            found_cols = _identify_columns(df.columns)
            
            # Select and Rename
            selected_data = {
                new_name: df[orig_col] 
                for orig_col, new_name in found_cols.items()
            }
            
            # Skip if vital data is missing
            if 'Datetime' not in selected_data:
                 print(f"Skipping {file}: Datetime column not found")
                 continue
                 
            new_df = pd.DataFrame(selected_data)

            # Parse Datetime with mixed format support
            new_df['Datetime'] = pd.to_datetime(new_df['Datetime'], format='mixed', utc=True)
            
            # Clean numeric columns
            if 'Speed' in new_df.columns:
                if new_df['Speed'].dtype == 'object':
                    new_df['Speed'] = new_df['Speed'].str.replace(',', '.', regex=False)
                new_df['Speed'] = pd.to_numeric(new_df['Speed'], errors='coerce')
            
            # Extract Temporal Features
            new_df['Date'] = new_df['Datetime'].dt.date
            new_df['Hour'] = new_df['Datetime'].dt.hour
            new_df['Month'] = new_df['Datetime'].dt.month
            new_df['Year'] = new_df['Datetime'].dt.year
            new_df['Weekday'] = new_df['Datetime'].dt.day_name()
            
            dfs.append(new_df)
                
        except Exception as e:
            print(f"Error loading {file}: {e}")

    if not dfs:
        return pd.DataFrame()
        
    return pd.concat(dfs, ignore_index=True)

def _identify_columns(columns):
    """
    Helper to map CSV columns to standard names based on substrings.
    """
    found_cols = {}
    for col in columns:
        c_lower = col.lower()
        if 'horodate' in c_lower and 'generated' in c_lower:
            found_cols[col] = 'Datetime'
        elif 'lane' in c_lower and 'rank' not in c_lower and 'col' in c_lower:
            found_cols[col] = 'Lane'
        elif 'direction_1_2' in c_lower:
            found_cols[col] = 'Direction'
        elif 'categorysterela_label' in c_lower:
            found_cols[col] = 'Category'
        elif 'category1' in c_lower:
            found_cols[col] = 'Category_SIREDO'
        elif c_lower.startswith('speed') and 'average' not in c_lower and 'validity' not in c_lower and 'delta' not in c_lower:
            found_cols[col] = 'Speed'
    return found_cols

def _get_unified_category(row):
    """
    Maps raw categories to [Vélos, Motos, VL, PL, Autre].
    """
    cat = str(row.get('Category', '')).lower()
    siredo = row.get('Category_SIREDO')
    
    if 'vélo' in cat:
        return 'Vélos'
    elif cat == 'moto':
        return 'Motos'
    elif siredo in [1, 12]:
        return 'VL'
    elif siredo in [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14]:
        return 'PL'
    return 'Autre'

def process_data(df):
    """
    Applies business logic: Category Unification, Day Types, Localization.
    """
    if df.empty:
        return df

    # Localize Days
    df['Weekday_FR'] = df['Weekday'].map(FRENCH_DAYS)
    
    # Unified Category
    df['UnifiedCategory'] = df.apply(_get_unified_category, axis=1)
    
    # Filter out 'Autre' immediately as requested for this dashboard
    df = df[df['UnifiedCategory'] != 'Autre'].reset_index(drop=True)
    
    # Define DayType (JO vs WE)
    # Using numpy where is faster and cleaner than apply
    df['DayType'] = np.where(df['Weekday'].isin(['Saturday', 'Sunday']), 'WE', 'JO')

    return df
